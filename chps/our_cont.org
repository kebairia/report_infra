* Our Contribution
** Main contribution of our project
** System Design
#+NAME: fig:overall_arch_old
#+caption: The old architecture approach
[[file:~/dox/wrk/pfe/docs/pfe_thesis/figures/overall_old.png]]

#+NAME: fig:overall_arch
#+caption: The overall architecture
#+ATTR_LATEX: :float sideways
[[file:~/dox/wrk/pfe/docs/pfe_thesis/figures/overall.png]]

#+NAME: fig:workflow
#+caption: The Workflow we followed
#+ATTR_LATEX: :float sideways
[[file:~/dox/wrk/pfe/docs/pfe_thesis/figures/workflow.png]]
** System implementation
*** Compute Units (Virtualization)
:PROPERTIES:
:CUSTOM_ID: virt
:END:

#+NAME: fig:virt_overall_virt
#+caption: Virtualization
#+ATTR_LATEX: :float sideways
[[file:~/dox/wrk/pfe/docs/pfe_thesis/figures/virt/overall_virt.png]]
*** Provisioning & Automation
:PROPERTIES:
:CUSTOM_ID: pr_and_auto
:END:
**** Vagrant 
**** Ansible

*** Container Orchestration
:PROPERTIES:
:CUSTOM_ID: cont_and_micro
:END:
**** Kubernetes 
**** Docker
*** High availability and Load Balancing
:PROPERTIES:
:CUSTOM_ID: ha
:END:
@@latex:\noindent@@
In order to achieve a higher level of high availability for our services and virtual machines,
we used a variety of technologies and methodologies, and we'll go through the different ways,
why we use them, and their benefits and drawback below.

#+NAME: fig:overall_ha_diagram
#+caption: High Availibility diagram
#+attr_latex: :width 12cm
[[file:~/dox/wrk/pfe/docs/pfe_thesis/figures/ha/overall_ha_diagram.png]]
**** Failover services 
@@latex:\noindent@@
When attempting to implement a highly available service, we must consider the type of service
for which we are attempting to do this.

When an active application, server, system, hardware component, or network in a computer network
fails or terminates abnormally, /failover/ switches to a redundant or standby computer server,
system, hardware component, or network.

Many services, like as acrshort:dns and acrshort:dhcp, have built-in failover, and we took advantage of this
to establish a highly available service, especially for crucial services like these.

This offload the burden of implementing the failover yourself, moreover, if a service implements
the failover itself, it is more likely to be well integrated and supported than if you do it
yourself with additional apps.
# additional apps means additional threats

{{{fourstar}}}
Take, for example, acrshort:dns service. When attempting to establish a acrshort:dns service,
you will find three sorts of dns servers: a Primary acrshort:dns server, a Secondary acrshort:dns
server, and a Cache-only server.
In this case, the secondary acrshort:dns servers serve as failover for the primary one, meaning
that if the primary acrshort:dns server fails, the backup DNS server takes over and handles
the queries.

We used the =named= package to implement our acrshort:dns server, and the package support the setup of
a primary and secondary acrshort:dns server.
With the use a priority concept in the configuration file, we can put a lower priority value
on the secondary dns server configuration file, and by that it will be a backup server.
On the client machines, and by using the =NetworkManager= utility, we can specify up to 4 acrshort:dns
servers in the =/etc/resolv.conf=, and this is more than enough.
#+name: code:master_dns_conf
#+begin_src sh
  include "/etc/named.rfc1912.zones";
  include "/etc/named.root.key";
  include "/etc/rndc.key";
  
  # Allow rndc management
  controls {
          inet 127.0.0.1 port 953 allow { 127.0.0.1; } keys { "rndc-key"; };
  };
  
  # Limit access to local network and esiedge LAN
  acl "clients" {
          127.0.0.0/8;
          192.168.3.0/24;
  };
  
  options {
          listen-on port 53 { 127.0.0.1; 192.168.3.2; }; ## MASTER
          listen-on-v6 { none; };
          directory 	"/var/named";
          dump-file 	"/var/named/data/cache_dump.db";
  
#+end_src
**** HAProxy & Keepalived
@@latex:\noindent@@
For the high availability based on load balancing, we used HAProxy .

HAProxy cite:anicas-2021-introd-haprox (High Availability Proxy) is a popular open source
[[acrshort:tcp][TCP]]/[[acrshort:http][HTTP]] (Layer 4 & 7) Load Balancer and proxy solution that runs on Linux, Solaris, and FreeBSD.
Its most typical application is to distribute workload across numerous servers to increase
server performance and reliability (e.g. web, application, database). 
HAProxy is well-known for its speed and efficiency, and it is used by a lot of well-known
websites, like [[https://www.github.com][GitHub]], [[https://stackoverflow.com/][Stack Overflow]], [[https://www.reddit.com/][Reddit]], [[https://twitter.com/][Twitter]], and so on.

HAProxy, as previously stated, works on both layer 4 and layer 7.
Layer 4 (transport layer) load balancing is the simplest approach to distribute network traffic
to multiple servers.
This kind of load balancing forwards user traffic based on [[acrshort:ip][IP]] range and port, but we can utilize
a more complex technique by using layer 7 (application layer) load balancing.
The load balancer can use layer 7 to forward requests to different backend servers depending
on the content of the user's request. 
This load balancing strategy allows you to run many web application servers under the same
domain and port; however, the key disadvantage is that it only works with [[acrshort:http][HTTP]].

{{{fourstar}}}
@@latex:\noindent@@
As demonstrated in autoref:fig:overall_ha_diagram , we used HAProxy and Keepalived with
Kubernetes nodes as a load balancer, with HAProxy forwarding and routing traffic for the
master nodes using the roundrobin algorithm to load balance traffic between the two masters.

When constructing load-balanced topologies, it's critical to consider the load balancer's
own availability as well as the availability of the real servers behind it.
We used the Keepalived service to accomplish this.
We used the Keepalived service to accomplish this.

Keepalived employs a set of health checkers to maintain and manage load balanced server pools
based on their health in a dynamic and adaptive manner.
The [[ac:vrrp]] achieves high availability. [[acrshort:vrrp][VRRP]] is a critical component of router failover.
Furthermore, keepalived adds a collection of hooks allowing for low-level and high-speed protocol interactions. 

In a nutshell, the workflow of the two services will be as follow:\\
If one of the master nodes fails, HAProxy will redirect traffic to the backup master node.
The use of a single HAProxy service raises the issue of a single point of failure.
In this scenario, we employed a backup HAProxy with a keepalived service that provides a
floating acrshort:ip address for the user to use while monitoring the master HAProxy with
Heartbeat signals and putting the backup HAProxy in master mode in the case of a master failure.
{{{clearpage}}}
**** Pacemaker
@@latex:\noindent@@
Pacemaker is a resource manager for high-availability clusters.

It achieves high availability for your cluster services (a.k.a. resources) by using Corosync's
messaging and membership features to detect and recover from node and resource level failures.

It can do this for clusters of virtually any size and has a sophisticated dependency model
that allows the administrator to precisely specify the relationships (both ordering and location)
between cluster resources.

{{{fourstar}}}
We employed pacemaker with Syslog servers and [[https://www.gluster.org/][GlusterFS]] (see Section {{{crefs(glusterfs)}}})
since these two services have special requirements that HAProxy cannot manage, and they do not
have their own built-in failover.

The amazing part about Pacemaker is that it allows you complete control over how your services
and resources work.
You can control every aspect of how the service behaves, from the type of resources it manages
to the specification of specific resources to run on specific servers...etc.
The extensibility and power of pacemaker came with a higher level of complexity to handle when
compared to the two forms of high availability implementation we described earlier, but it is
still a wonderful choice for building a robust infrastructure.
*** Network Configuration

#+NAME: fig:network_net_types_and_confs
#+caption: Network types that we used and their configurations
#+attr_latex: :width 9cm
[[file:~/dox/wrk/pfe/docs/pfe_thesis/figures/network/net_conf_and_mnm_configuration.png]]
**** Management Network
**** Public Network & Bridged Network
#+NAME: fig:network_public_bridge_network
#+caption: How Public interfaces and bridge network work
#+attr_latex: :width 8cm
[[file:~/dox/wrk/pfe/docs/pfe_thesis/figures/network/network_bridge_private_pfe.png]]
**** Private Network
*** Storage
:PROPERTIES:
:CUSTOM_ID: storage
:END:
**** GlusterFS
:PROPERTIES:
:CUSTOM_ID: glusterfs
:END:
@@latex:\noindent@@
Gluster is a distributed file system that combines disk storage resources from numerous hosts
into a single global namespace.
With no vendor lock-in, enterprises can grow capacity, performance, and availability on demand
across on-premises, public cloud, and hybrid environments.
Thousands of companies in the media, healthcare, government, education, and financial services
use [[https://www.gluster.org/][GlusterFS]] in their production.

One of the advantages of utilizing GlusterFS is that it runs natively on most linux-based
systems, including the one we use, and can be used in conjunction with other technologies
such as [[acrshort:nfs][NFS]] for remote file access.
**** NFS
@@latex:\noindent@@
acrshort:nfs is a well known and venerable network protocol created by Sun Microsystems (Sun)
in 1984, and it is widely used.
[[acrshort:nfs]]v4 is the latest version of the protocol. It fully reconsiders its semantic and the way acrshort:nfs
can be used.
The fundamental function of acrshort:nfs is to allows a user on a client computer to access
files via a computer network in the same way that local storage is accessed.


Many businesses still rely on NFS to access data from a variety of operating systems and applications.
Glusterfs has native acrshort:nfs support (referred to as Gluster-NFS), which serves as a GlusterFS
client on the same node as the GlusterFS server, but it support only [[acrshort:nfs]]v3 protocol.

Also, native acrshort:nfs does not support the extending feature that GlusterFS provides.
In this case, we used a technology called [[https://nfs-ganesha.github.io/][NFS-Ganesha]], which attempts to implement this
feature and overcome the limitations of classical acrshort:nfs.

**** NFS-Ganesha
 NFS-Ganesha is a user-space [[acrshort:nfs][NFS]] file server that supports [[acrshort:nfs][NFS]]v3, v4, v4.1, and p[[acrshort:nfs][NFS]] protocols.
 It has a FUSE-compatible [[Ac:fsal][File System Abstraction Layer (FSAL)]] that allows file-system developers
 to add their own storage mechanism and access it from any [[acrshort:nfs][NFS]] client. NFS-Ganesha may access
 FUSE filesystems directly through its [[acrshort:fsal][FSAL]], avoiding the need for data to be copied to or
 from the kernel, potentially improving response times. 
*** Security
:PROPERTIES:
:CUSTOM_ID: sec
:END:
**** OpenVPN 
*** Test and Results
*** Conclusion

* Local Variables                                           :noexport:ignore:
# Local Variables:
# mode: org
# org-export-allow-bind-keywords: t
# eval: (setq display-fill-column-indicator-column 100)
# eval: (display-fill-column-indicator-mode)
# eval: (flyspell-mode t)
# End:
